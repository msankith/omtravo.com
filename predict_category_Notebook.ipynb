{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "# import tensorflow as tf\n",
    "from scipy import sparse\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import svm\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training File  -f\n"
     ]
    }
   ],
   "source": [
    "train_file, test_file = sys.argv[1:]\n",
    "print(\"Loading training File \",train_file)\n",
    "data =json.load(open(\"comtravo_challenge_train.json\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get Lables \n",
    "labels_count={}\n",
    "for i in range(len(data)):\n",
    "    for label in data[i]['labels'].keys():\n",
    "        if label in labels_count:\n",
    "            labels_count[label]+=1\n",
    "        else:\n",
    "            labels_count[label]=1\n",
    "lables_ids={}\n",
    "for lab,itr  in zip(labels_count.keys(),range(len(labels_count.keys()))):\n",
    "    lables_ids[lab]=itr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'booking': 4470,\n",
       " 'cancelation': 222,\n",
       " 'issues': 23,\n",
       " 'negotiation': 793,\n",
       " 'other': 1654,\n",
       " 'rebooking': 568}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#NER tokens -> required for feature mapping\n",
    "ner={}\n",
    "for i in range(len(data)):\n",
    "    for token in data[i]['tokens']:\n",
    "        ner[token['rner']]=1   \n",
    "nerFeaturePosition={}\n",
    "for value,pos in zip(ner.keys(),range(len(ner.keys()))):\n",
    "    nerFeaturePosition[value]=pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#features \n",
    "#where_1 = is_body\n",
    "#where_2 = is_subject\n",
    "#shape_1 = begins_with_capital\n",
    "#shape_2 = contains_colon\n",
    "#shape_3 = contains_hyphen\n",
    "#shape_4 = contains_d\n",
    "#start   = is_begining (is 1 if its positon is less than 10)\n",
    "#ner = 2 placeholder for every nerType (so 2*24) + 1 for other\n",
    "#feature vector for every token = 2+4+1+49 = 56\n",
    "\n",
    "maxTokenLength = 2600 #required for padding  If text contains more than 2600 tokens, it will be ignored. if less thant it ll be padded with 0\n",
    "tokenFeaturesCount=56\n",
    "\n",
    "def featureMapping(tokens):\n",
    "    featureVector = np.zeros([maxTokenLength,tokenFeaturesCount])\n",
    "\n",
    "    ##Feature Vector is used for CNN -> which I wanted to experiment\n",
    "\n",
    "    columns=[]\n",
    "    for tok,itr in zip(tokens,range(len(tokens))):\n",
    "        if itr>= maxTokenLength:\n",
    "            break\n",
    "        if tok['where']=='body':\n",
    "            featureVector[itr][0]=1\n",
    "            columns.append(itr*tokenFeaturesCount+0)\n",
    "        \n",
    "        if tok['where']=='subject':\n",
    "            featureVector[itr][1]=1\n",
    "            columns.append(itr*tokenFeaturesCount+1)\n",
    "        \n",
    "        if tok['shape'].startswith('X'):\n",
    "            featureVector[itr][2]=1\n",
    "            columns.append(itr*tokenFeaturesCount+2)\n",
    "            \n",
    "        if ':' in tok['shape']:\n",
    "            featureVector[itr][3]=1\n",
    "            columns.append(itr*tokenFeaturesCount+3)\n",
    "            \n",
    "        if '-' in tok['shape']:\n",
    "            featureVector[itr][4]=1\n",
    "            columns.append(itr*tokenFeaturesCount+4)\n",
    "        \n",
    "        if 'd' in tok['shape']:\n",
    "            featureVector[itr][5]=1\n",
    "            columns.append(itr*tokenFeaturesCount+5)\n",
    "        \n",
    "        if tok['start'] < 10:\n",
    "            featureVector[itr][6]=1\n",
    "            columns.append(itr*tokenFeaturesCount+6)\n",
    "            \n",
    "        nerFeature = 7+int(nerFeaturePosition[tok['rner']])\n",
    "        columns.append(itr*tokenFeaturesCount+nerFeature)\n",
    "        featureVector[itr][nerFeature]=1\n",
    "        \n",
    "    return columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#splitting data into train and test (75 percent training data,25% validation data)\n",
    "random.shuffle(data)\n",
    "splitPoint=int(len(data)*0.75)\n",
    "trainingData = data[:splitPoint]\n",
    "validationData=data[splitPoint:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dataToFeature(data,testSet=False):\n",
    "    colm=[]\n",
    "    row = []\n",
    "    d =[]\n",
    "    tar=[]\n",
    "    batch=len(data)\n",
    "    print(\"Genearating feature vector \",batch)\n",
    "    for i in range(len(lables_ids)):\n",
    "        tar.append(np.zeros(batch))\n",
    "\n",
    "    for i in range(batch):\n",
    "        if i %1500==0:\n",
    "            print (i,\" loaded\")\n",
    "        fea=featureMapping(data[i]['tokens'])\n",
    "        for ele in fea:\n",
    "            colm.append(ele)\n",
    "            row.append(i)\n",
    "            d.append(1)\n",
    "        \n",
    "        if testSet == True:\n",
    "            continue\n",
    "        for lab in data[i]['labels']:\n",
    "            tar[lables_ids[lab]][i]=1\n",
    " \n",
    "    #dummy variable to make feature length of test/validation/train same size\n",
    "    row.append(i)\n",
    "    d.append(0)\n",
    "    colm.append(maxTokenLength*tokenFeaturesCount)\n",
    "    return sparse.csr_matrix((d,(row,colm))),tar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genearating feature vector  1893\n",
      "0  loaded\n",
      "1500  loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/sparse/coo.py:200: VisibleDeprecationWarning: `rank` is deprecated; use the `ndim` attribute or function instead. To find the rank of a matrix see `numpy.linalg.matrix_rank`.\n",
      "  if np.rank(self.data) != 1 or np.rank(self.row) != 1 or np.rank(self.col) != 1:\n",
      "/usr/lib/python3/dist-packages/scipy/sparse/compressed.py:130: VisibleDeprecationWarning: `rank` is deprecated; use the `ndim` attribute or function instead. To find the rank of a matrix see `numpy.linalg.matrix_rank`.\n",
      "  if np.rank(self.data) != 1 or np.rank(self.indices) != 1 or np.rank(self.indptr) != 1:\n"
     ]
    }
   ],
   "source": [
    "feature_valid,label_valid=dataToFeature(validationData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genearating feature vector  5678\n",
      "0  loaded\n",
      "1500  loaded\n",
      "3000  loaded\n",
      "4500  loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/sparse/coo.py:200: VisibleDeprecationWarning: `rank` is deprecated; use the `ndim` attribute or function instead. To find the rank of a matrix see `numpy.linalg.matrix_rank`.\n",
      "  if np.rank(self.data) != 1 or np.rank(self.row) != 1 or np.rank(self.col) != 1:\n",
      "/usr/lib/python3/dist-packages/scipy/sparse/compressed.py:130: VisibleDeprecationWarning: `rank` is deprecated; use the `ndim` attribute or function instead. To find the rank of a matrix see `numpy.linalg.matrix_rank`.\n",
      "  if np.rank(self.data) != 1 or np.rank(self.indices) != 1 or np.rank(self.indptr) != 1:\n"
     ]
    }
   ],
   "source": [
    "feature_train,label_train=dataToFeature(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lables_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buidling model for  booking 4\n",
      "[LibSVM]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/sparse/compressed.py:130: VisibleDeprecationWarning: `rank` is deprecated; use the `ndim` attribute or function instead. To find the rank of a matrix see `numpy.linalg.matrix_rank`.\n",
      "  if np.rank(self.data) != 1 or np.rank(self.indices) != 1 or np.rank(self.indptr) != 1:\n"
     ]
    }
   ],
   "source": [
    "models ={}\n",
    "yPred={}\n",
    "for label in lables_ids:\n",
    "    labelId = lables_ids[label]\n",
    "    print(\"Buidling model for \",label,labelId)\n",
    "    ##Cross Validation and Hyper parameter tuning\n",
    "    #     parameter_candidates = [\n",
    "#   {'C': [1, 4, 16, 32,64,1024], 'kernel': ['linear']}\n",
    "# ]\n",
    "#     clf = GridSearchCV(estimator=svm.SVC(), param_grid=parameter_candidates, n_jobs=3,scoring=f1_scorer)\n",
    "#     clf.fit(feature_train,label_train[labelId])\n",
    "    models[label] = svm.SVC(kernel='linear', C = 1.0,probability=True,verbose=True)\n",
    "    models[label].fit(feature_train,label_train[labelId])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Before threshold tuning \")\n",
    "yPred={}\n",
    "yPred_prob={}\n",
    "for label in lables_ids:\n",
    "    labelId = lables_ids[label]\n",
    "    yPred[label]=models[label].predict(feature_valid)\n",
    "    yPred_prob[label]=models[label].predict_proba(feature_valid)\n",
    "    print(label,\" F-Score \" ,f1_score(label_valid[labelId],yPred[label]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"After threshodl tuning\")\n",
    "bestThreshold={}\n",
    "for label in lables_ids:\n",
    "    labelId=lables_ids[label]\n",
    "    bestThreshold[label]=0.5\n",
    "    bestFScore=0\n",
    "    thresSorted =sorted(set([round(x,3) for x in sorted(yPred_prob[label][:,1])]))\n",
    "    for thres in thresSorted:\n",
    "        tempFscore=f1_score(label_valid[labelId],yPred_prob[label][:,1]>thres)\n",
    "        if tempFscore>bestFScore:\n",
    "            bestFScore=tempFscore\n",
    "            bestThreshold[label]= thres\n",
    "    print(label,\" Best F-score \",bestFScore,\" Optimal threshold thres\",bestThreshold[label])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Loading Test data\")\n",
    "data_test = json.load(open(\"comtravo_challenge_test.json\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(\"Test Data Feature gen\")\n",
    "feature_test,temp=dataToFeature(data_test,testSet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Predicting Labels\")\n",
    "yPred_test={}\n",
    "yPred_prob_test={}\n",
    "for label in lables_ids:\n",
    "    labelId = lables_ids[label]\n",
    "    yPred_test[label]=models[label].predict(feature_test)\n",
    "    yPred_prob_test[label]=models[label].predict_proba(feature_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(data_test)):\n",
    "    data_test[i]['labels']={}\n",
    "    noneLabel=True\n",
    "    for label in lables_ids:\n",
    "        labelId = lables_ids[label]\n",
    "#         print(label,yPred_prob_test[label][i][1],bestThreshold[label])\n",
    "        if yPred_prob_test[label][i][1]>bestThreshold[label]:\n",
    "            noneLabel=False\n",
    "            data_test[i]['labels'][label]=yPred_prob_test[label][i][1]\n",
    "    if noneLabel :\n",
    "        data_test[i]['labels']['others']=1\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for i in range(len(data_test)):\n",
    "# #     print (len(data[i]['labels'].keys()))\n",
    "#     if len(data_test[i]['labels'].keys()) >1:\n",
    "#         print(data_test[i]['labels'])\n",
    "# #     print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"comtravo_challenge_test.json has been created\")\n",
    "json.dump(data_test,open(\"comtravo_challenge_test.json\",\"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_request(r):\n",
    "    \"\"\"Print a request in a human readable format to stdout\"\"\"\n",
    "    def fmt_token(t):\n",
    "        return t['shape'] + t['after']\n",
    "\n",
    "    print('Subject: ' + ''.join(map(fmt_token, filter(\n",
    "        lambda x: x['where'] == 'subject', r['tokens']))))\n",
    "    print(''.join(map(fmt_token, filter(\n",
    "        lambda x: x['where'] == 'body', r['tokens']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_request(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
